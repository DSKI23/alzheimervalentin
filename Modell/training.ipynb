{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r \"https://raw.githubusercontent.com/DSKI23/alzheimervalentin/refs/heads/main/requirements.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "from huggingface_hub import notebook_login, login\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import pickle\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking enviorment variables, so that the github actions can execute the file as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if 'state' in os.environ:\n",
    "    state = int(os.environ['state'])\n",
    "else:\n",
    "    state = 42\n",
    "\n",
    "np.random.seed(state)\n",
    "\n",
    "if 'huggingface_token' in os.environ:\n",
    "    login(token= os.environ['huggingface_token'])\n",
    "else:\n",
    "    notebook_login()\n",
    "\n",
    "if 'wandb_api_key' in os.environ:\n",
    "    wandb.login(key = os.environ['wandb_api_key'])\n",
    "else:\n",
    "    wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. load the dataset from huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"DS23-KI-Projekt/alzheimerdataset_split\")\n",
    "df = dataset['train'].to_pandas()\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Zielvariable und Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    try:\n",
    "        df = dataset[\"train\"].to_pandas()\n",
    "        target = \"Alzheimer’s Diagnosis\"\n",
    "        features = [col for col in df.columns if col != target]\n",
    "        return df[features], df[target]\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "def train_rf(X_train, X_test, y_train, y_test, n_estimators):\n",
    "    rf_model = RandomForestClassifier(n_estimators=n_estimators, random_state=42)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = rf_model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    class_report = classification_report(y_test, y_pred, output_dict=True)  # JSON format for WandB\n",
    "    cfm = confusion_matrix(y_test, y_pred).tolist()  # Convert to list for WandB logging\n",
    "\n",
    "    return rf_model, accuracy, class_report, cfm\n",
    "\n",
    "#  Main function\n",
    "def main():\n",
    "    # Initialize WandB\n",
    "    wandb.init(entity=\"zhannalialko-dhbw-mosbach\", project=\"alzheimer-rf\", name=\"RF-Training\")\n",
    "\n",
    "    # Load dataset\n",
    "    X, y = load_data()\n",
    "\n",
    "    # Split dataset (80% train, 20% test)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "    best_model = None\n",
    "    best_accuracy = 0\n",
    "\n",
    "    #  Train over 10 epochs with increasing estimators\n",
    "    for epoch in range(1, 11):\n",
    "        n_estimators = epoch * 10  # Start with 10 trees, increase to 100\n",
    "\n",
    "        rf_model, accuracy, class_report, cfm = train_rf(X_train, X_test, y_train, y_test, n_estimators)\n",
    "\n",
    "        # Save best model\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_model = rf_model\n",
    "\n",
    "        # Log results in WandB\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"accuracy\": accuracy,\n",
    "            \"n_estimators\": n_estimators,\n",
    "            \"classification_report\": class_report,\n",
    "            \"confusion_matrix\": cfm\n",
    "        })\n",
    "\n",
    "        print(f\"Epoch {epoch} | n_estimators={n_estimators} | Accuracy={accuracy:.4f}\")\n",
    "\n",
    "    #  Save the best model\n",
    "    model_dir = \"random_forest_alzheimer\"\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    with open(f\"{model_dir}/best_rf_model.pkl\", \"wb\") as file:\n",
    "        pickle.dump(best_model, file)\n",
    "\n",
    "    print(f\"Best model saved with accuracy: {best_accuracy:.4f}\")\n",
    "\n",
    "    # Finish WandB logging\n",
    "    wandb.finish()\n",
    "\n",
    "    return best_model\n",
    "best_model = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('random_forest_alzheimer'):\n",
    "    os.makedirs('random_forest_alzheimer')\n",
    "\n",
    "with open('random_forest_alzheimer.pkl', 'wb') as file:\n",
    "    pickle.dump(best_model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modell speichern .pkl-Datei\n",
    "model_filename = \"random_forest_alzheimer.pkl\"\n",
    "\n",
    "# Repository-Name auf Hugging Face (anpassen!)\n",
    "hf_repo_name = \"DS23-KI-Projekt/KI-Modell\"\n",
    "\n",
    "# Falls das Repository nicht existiert, erstelle es\n",
    "api = HfApi()\n",
    "api.create_repo(repo_id=hf_repo_name, exist_ok=True)\n",
    "\n",
    "# Modell hochladen\n",
    "api.upload_file(\n",
    "    path_or_fileobj=model_filename,  # Die gespeicherte Modell-Datei\n",
    "    path_in_repo=model_filename,  # Name im Hugging Face Repo\n",
    "    repo_id=hf_repo_name,  # Dein Repository\n",
    "    commit_message=\"Upload des Random Forest Modells für Alzheimer-Diagnose\"\n",
    ")\n",
    "\n",
    "\n",
    "api.upload_file(\n",
    "    path_or_fileobj=model_filename,  # Die gespeicherte Modell-Datei\n",
    "    path_in_repo=model_filename,  # Name im Hugging Face Repo\n",
    "    repo_id=\"DS23-KI-Projekt/alzheimers-screening-assistant\",  # Dein Repository\n",
    "    commit_message=\"Upload des Random Forest Modells für Alzheimer-Diagnose\"\n",
    ")\n",
    "\n",
    "print(f\" Modell hochgeladen auf Hugging Face: https://huggingface.co/{hf_repo_name}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
